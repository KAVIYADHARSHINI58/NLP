{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "IIeYi41tRTqb"
      },
      "outputs": [],
      "source": [
        "text=\"My name is kaviya\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTxOJknHSLVz",
        "outputId": "4ca4a23a-a03f-446a-e288-d10ff5a258a9"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My', 'name', 'is', 'kaviya']"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"My name is kaviya . I am studying in bitsathy .\""
      ],
      "metadata": {
        "id": "7QW1cxc2STNs"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.split('.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ql-I6o_sS6h7",
        "outputId": "da3af974-4ec8-457c-8eb5-c31cb8c1e94e"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My name is kaviya ', ' I am studying in bitsathy ', '']"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Regex"
      ],
      "metadata": {
        "id": "KJ-Fun23UXn1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "x4a5jHzoS_gM"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text= \"Who are you ? My name is kaviya . I am studying in bitsathy .\""
      ],
      "metadata": {
        "id": "eXkRIAB5TdJ2"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = re.compile('[.?!]').split(text)"
      ],
      "metadata": {
        "id": "RjmxKasPTsCf"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyETwKkWU9xe",
        "outputId": "820fc7fe-dad8-41e1-d983-f0958e654a3b"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Who are you ', ' My name is kaviya ', ' I am studying in bitsathy ', '']"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "2TB5fW10TWeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk"
      ],
      "metadata": {
        "id": "vRHD-syEVCoi"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "Jc5bsX3oVKri"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKu-qzzOVcSR",
        "outputId": "b75c1b1e-9644-4d4c-ac76-6340ca21e6e2"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence =\"My name is kaviya . I am studying in bitsathy .\""
      ],
      "metadata": {
        "id": "OWOKpHf-VqUZ"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvdR_FN7WCyQ",
        "outputId": "4c115f4a-02b0-417a-d6eb-aa6aeda4dee6"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My',\n",
              " 'name',\n",
              " 'is',\n",
              " 'kaviya',\n",
              " '.',\n",
              " 'I',\n",
              " 'am',\n",
              " 'studying',\n",
              " 'in',\n",
              " 'bitsathy',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize  import sent_tokenize"
      ],
      "metadata": {
        "id": "muE3l05nWH83"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJRe7sToW4Nc",
        "outputId": "4f98d7e3-efeb-4ff9-da35-fa8491c74b26"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My name is kaviya .', 'I am studying in bitsathy .']"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "sentence = \"If you are positive, you’ll see opportunities instead of obstacles\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQ-VbworTpPm",
        "outputId": "87f52136-fbb2-486c-8b4b-341d58e7b33e"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['If',\n",
              " 'you',\n",
              " 'are',\n",
              " 'positive',\n",
              " ',',\n",
              " 'you',\n",
              " '’',\n",
              " 'll',\n",
              " 'see',\n",
              " 'opportunities',\n",
              " 'instead',\n",
              " 'of',\n",
              " 'obstacles']"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "sentence = \"My dear friend, clear your mind of can’t\"\n",
        "wordpunct_tokenize(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WClJNGmvUsek",
        "outputId": "023fd097-953f-479d-dbae-8187ffd669a4"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My', 'dear', 'friend', ',', 'clear', 'your', 'mind', 'of', 'can', '’', 't']"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dj8UlM02WZDP",
        "outputId": "826ef23a-f3ac-4cee-8887-8651080ed29a"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "sentence = 'My name is Kaviya. I am studying in bit sathy'\n",
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('bit','sathy'))\n",
        "tokenizer.tokenize(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9ZreKE1Wmvt",
        "outputId": "ef7889f4-c005-464b-af83-4e6cb3ea3539"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['My', 'name', 'is', 'Kaviya', '.', 'I', 'am', 'studying', 'in', 'bit_sathy']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming"
      ],
      "metadata": {
        "id": "fz2-9GxhW-W7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "INnSlybfW8V-"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter=PorterStemmer()"
      ],
      "metadata": {
        "id": "T7OS7bxZXOrm"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words=['wait','waiting','waited','waits']"
      ],
      "metadata": {
        "id": "f2s1RppcXVps"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word, \"-->\",porter.stem(word))\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tXX-ZjPrXoGU",
        "outputId": "df143cfe-c351-4bfd-ec55-892c48c11961"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait --> wait\n",
            "waiting --> wait\n",
            "waited --> wait\n",
            "waits --> wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "cZrdaapBYG1F"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer(language=\"english\")"
      ],
      "metadata": {
        "id": "kbuoGYBFZcIM"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word, \"-->\",snowball.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spJq9x9tZ5hl",
        "outputId": "cd328127-ea35-4654-cc56-4aca6b8c1680"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wait --> wait\n",
            "waiting --> wait\n",
            "waited --> wait\n",
            "waits --> wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "words = ['jumping','jumps','jumped','sleeping','sleeps','slept']\n",
        "for word in words:\n",
        "     print(word,\"--->\",lancaster.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fq55QC62VfpZ",
        "outputId": "6e7d4ee0-9422-4914-f701-76c321c0ab25"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jumping ---> jump\n",
            "jumps ---> jump\n",
            "jumped ---> jump\n",
            "sleeping ---> sleep\n",
            "sleeps ---> sleep\n",
            "slept ---> slept\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regexp = RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "words = ['driving','drives','drive','comfortable']\n",
        "for word in words:\n",
        "      print(word,\"--->\",regexp.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eP1oijDCYhYe",
        "outputId": "b4482786-af85-40e7-f1e2-f96934b6277c"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "driving ---> driv\n",
            "drives ---> drive\n",
            "drive ---> driv\n",
            "comfortable ---> comfort\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "JsGUKtBhgf1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word"
      ],
      "metadata": {
        "id": "3ioS4ERjaMOu"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDxi45ZOg5aa",
        "outputId": "c2ea518c-255e-405d-dd8f-26bb9ea2db65"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "listofwords = ['waiting', 'plays', 'flies', 'borrowed'] "
      ],
      "metadata": {
        "id": "BWtOEZ1ahE4z"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in listofwords:\n",
        "   w = Word(word)\n",
        "   print(word + '-->' + w.lemmatize())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_SLai1IhY1C",
        "outputId": "1a9c54b9-88b9-4c6e-8b9e-bff532bb2ae5"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting-->waiting\n",
            "plays-->play\n",
            "flies-->fly\n",
            "borrowed-->borrowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "Xj-S8tFVhzb0"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wn1 = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ftl-aVaOiIqh"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for words in listofwords:\n",
        "     print(words + '-->' + wn1.lemmatize(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsRV-PyxiTL1",
        "outputId": "6b5b9875-8f52-4fd9-cd39-89bc6350b322"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "waiting-->waiting\n",
            "plays-->play\n",
            "flies-->fly\n",
            "borrowed-->borrowed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc = nlp('Mary and Samantha realized that Joe was waiting at the train station after they left on the bus')\n",
        "tokens = []\n",
        "for token in doc:\n",
        "     tokens.append(token)\n",
        "print(\"Tokens --->\",tokens)\n",
        "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
        "print(\"Lemmatized sentence --->\",lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0vWS5bcadVg",
        "outputId": "112e8bce-c8cc-4c8d-90a0-9c9242a87930"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens ---> [Mary, and, Samantha, realized, that, Joe, was, waiting, at, the, train, station, after, they, left, on, the, bus]\n",
            "Lemmatized sentence ---> Mary and Samantha realize that Joe be wait at the train station after -PRON- leave on the bus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pattern"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQakAbcAhpm2",
        "outputId": "0016e3c3-fddf-401a-94ab-398787286988"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pattern in /usr/local/lib/python3.7/dist-packages (3.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from pattern) (4.2.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.4.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern) (4.6.3)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.7/dist-packages (from pattern) (0.8.11)\n",
            "Requirement already satisfied: mysqlclient in /usr/local/lib/python3.7/dist-packages (from pattern) (2.1.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pattern) (0.16.0)\n",
            "Requirement already satisfied: backports.csv in /usr/local/lib/python3.7/dist-packages (from pattern) (1.0.7)\n",
            "Requirement already satisfied: pdfminer.six in /usr/local/lib/python3.7/dist-packages (from pattern) (20220319)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from pattern) (3.2.5)\n",
            "Requirement already satisfied: cherrypy in /usr/local/lib/python3.7/dist-packages (from pattern) (18.6.1)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.7/dist-packages (from pattern) (6.0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pattern) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pattern) (2.23.0)\n",
            "Requirement already satisfied: zc.lockfile in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (2.0)\n",
            "Requirement already satisfied: portend>=2.1.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (3.1.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.12.0)\n",
            "Requirement already satisfied: cheroot>=8.2.1 in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (8.6.0)\n",
            "Requirement already satisfied: jaraco.collections in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern) (3.5.1)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (3.5.0)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.15.0)\n",
            "Requirement already satisfied: tempora>=1.8 in /usr/local/lib/python3.7/dist-packages (from portend>=2.1.1->cherrypy->pattern) (5.0.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.1)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.7/dist-packages (from feedparser->pattern) (1.0.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->pattern) (3.2.1)\n",
            "Requirement already satisfied: jaraco.text in /usr/local/lib/python3.7/dist-packages (from jaraco.collections->cherrypy->pattern) (3.7.0)\n",
            "Requirement already satisfied: jaraco.context>=4.1 in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (4.1.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (5.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern) (3.8.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (3.0.4)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern) (37.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern) (2.21)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pattern) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern) (57.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pattern \n",
        "from pattern.en import lemma,lexeme\n",
        "from pattern.en import parse\n",
        "sentence = \"He was waiting at the train station after they left on the bus\"\n",
        "lemmatized_sentence = \" \".join([lemma(word) for word in sentence.split()])\n",
        "print(\"Lemmatized sentence --->\",lemmatized_sentence)\n",
        "all_lemmas_for_each_word = [lexeme(wd) for wd in sentence.split()]\n",
        "print(all_lemmas_for_each_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgwlxR3Xh52I",
        "outputId": "150968bc-e53e-4c74-90a7-94e8ceb229f0"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized sentence ---> he be wait at the train station after they left on the bus\n",
            "[['he', 'hes', 'hing', 'hed'], ['be', 'am', 'are', 'is', 'being', 'was', 'were', 'been', 'am not', \"aren't\", \"isn't\", \"wasn't\", \"weren't\"], ['wait', 'waits', 'waiting', 'waited'], ['at', 'ats', 'ating', 'ated'], ['the', 'thes', 'thing', 'thed'], ['train', 'trains', 'training', 'trained'], ['station', 'stations', 'stationing', 'stationed'], ['after', 'afters', 'afterring', 'afterred'], ['they', 'theys', 'theying', 'theyed'], ['left'], ['on', 'ons', 'oning', 'oned'], ['the', 'thes', 'thing', 'thed'], ['bus', 'busses', 'bussing', 'bussed']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import lemmatize\n",
        "sentence =\"He was waiting at the train station after they left on the bus\"\n",
        "lemmatized_sentence = [word.decode('utf-8').split('.')[0] for word in lemmatize(sentence)]\n",
        "print(lemmatized_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0lgHVoQjUPR",
        "outputId": "82baf957-7a34-43ff-cd4d-34209f7efba3"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['be/VB', 'wait/VB', 'train/NN', 'station/NN', 'left/VB', 'bus/NN']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine Similarity"
      ],
      "metadata": {
        "id": "gYww6Le7kA2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import en_core_web_sm"
      ],
      "metadata": {
        "id": "0bBjCPwmik31"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = en_core_web_sm.load()"
      ],
      "metadata": {
        "id": "utyvRx0Sutnj"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = nlp(\"How can I check my account balance\")\n",
        "sent2 = nlp(\"I want to check my account balance\")\n",
        "print(sent2.similarity(sent1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1ogkvqOu1Qo",
        "outputId": "73eb2070-f713-4c2f-9b11-afcd5c94202a"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7998775843063977\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords"
      ],
      "metadata": {
        "id": "OpRA1IjikFQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfXeO-4FvO-a",
        "outputId": "8c3cdcb5-9012-4f92-a99f-a9ae3ea44af4"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "EN-CS1PGwFIc"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ME6Y-b0wWuy",
        "outputId": "596ba8d7-31ed-4202-fe56-f1243710f982"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9OgJQZxweVW",
        "outputId": "52d0082b-8111-49c0-aa03-1029785f7fa7"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokens_1 = word_tokenize(\"How can I check my account balance\")\n",
        "word_tokens_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkONtZe3wpDd",
        "outputId": "f0d693e5-f8db-462a-fd36-0ecca96f83f6"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['How', 'can', 'I', 'check', 'my', 'account', 'balance']"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_sent_1 = [w for w in word_tokens_1 if w.lower() not in stop_words]\n",
        "new_sent_1 = ' '.join(str(elem) for elem in new_sent_1)\n",
        "print(new_sent_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF5lKbtZxYe6",
        "outputId": "98a562b3-b9d9-44f9-f15f-42da902dc00a"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "check account balance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_token_2 = word_tokenize(\"I want to check my account balance\")\n",
        "new_sent_2 = [w for w in word_token_2 if w.lower() not in stop_words]\n",
        "new_sent_2 = ' '.join(str(elem) for elem in new_sent_2)\n",
        "print(new_sent_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80OGf5ityhlD",
        "outputId": "da88f432-d9b6-4164-d7db-bc9dba66149c"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "want check account balance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " new_sent_1 = nlp(new_sent_1)\n",
        " new_sent_2 = nlp(new_sent_2)\n",
        " print(new_sent_2.similarity(new_sent_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvle4qDcz5cW",
        "outputId": "7c557779-dc28-4560-d501-add94014992a"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9003613092570767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  \"__main__\", mod_spec)\n"
          ]
        }
      ]
    }
  ]
}